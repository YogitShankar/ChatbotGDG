{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OSOrcZN0VTBz",
        "outputId": "b82b252a-7e93-4cd3-8bb9-ddc36c8f64a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a contest ID (e.g., 2043): 2043\n",
            "\n",
            "Starting to scrape contest: 2043\n",
            "Scraping: https://codeforces.com/contest/2043/problem/A\n",
            "Successfully scraped: A. Coin Transformation\n",
            "Scraping: https://codeforces.com/contest/2043/problem/B\n",
            "Successfully scraped: B. Digits\n",
            "Scraping: https://codeforces.com/contest/2043/problem/C\n",
            "Successfully scraped: C. Sums on Segments\n",
            "Scraping: https://codeforces.com/contest/2043/problem/D\n",
            "Successfully scraped: D. Problem about GCD\n",
            "Scraping: https://codeforces.com/contest/2043/problem/E\n",
            "Successfully scraped: E. Matrix Transformation\n",
            "Scraping: https://codeforces.com/contest/2043/problem/F\n",
            "Successfully scraped: F. Nim\n",
            "Scraping: https://codeforces.com/contest/2043/problem/G\n",
            "Successfully scraped: G. Problem with Queries\n"
          ]
        }
      ],
      "source": [
        "from urllib.request import Request, urlopen\n",
        "from urllib.error import HTTPError, URLError\n",
        "from bs4 import BeautifulSoup\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "\n",
        "# Create directories for storing scraped data\n",
        "os.makedirs(\"problems\", exist_ok=True)\n",
        "os.makedirs(\"metadata\", exist_ok=True)\n",
        "\n",
        "def scrape_problem(contest_id, problem_id):\n",
        "    url = f\"https://codeforces.com/contest/{contest_id}/problem/{problem_id}\"\n",
        "    print(f\"Scraping: {url}\")\n",
        "\n",
        "    req = Request(\n",
        "        url=url,\n",
        "        headers={\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "        }\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        response = urlopen(req)\n",
        "        page_content = response.read().decode(\"utf-8\")\n",
        "        soup = BeautifulSoup(page_content, \"html.parser\")\n",
        "\n",
        "        # Extract problem title\n",
        "        problem_title = soup.find(\"div\", class_=\"title\")\n",
        "        problem_title = problem_title.get_text(strip=True) if problem_title else None\n",
        "\n",
        "        # If no problem title is found, consider the problem does not exist\n",
        "        if not problem_title:\n",
        "            print(f\"No problem found for {problem_id}. Stopping.\")\n",
        "            return False  # Stop processing further problem IDs\n",
        "\n",
        "        # Extract problem statement\n",
        "        problem_statement_div = soup.find(\"div\", class_=\"problem-statement\")\n",
        "        problem_statement = \"\"\n",
        "        if problem_statement_div:\n",
        "            for section in problem_statement_div.find_all([\"div\", \"p\", \"pre\"]):\n",
        "                if section.name == \"div\" and \"header\" in section.get(\"class\", []):\n",
        "                    problem_statement += f\"\\n\\n{section.get_text(strip=True)}\\n\"\n",
        "                elif section.name == \"pre\":\n",
        "                    problem_statement += f\"\\n{section.get_text()}\\n\"\n",
        "                else:\n",
        "                    problem_statement += f\"{section.get_text(strip=True)} \"\n",
        "\n",
        "        # Replace $$$ with single $\n",
        "        problem_statement = problem_statement.replace(\"$$$\", \"\")\n",
        "\n",
        "        # Extract metadata\n",
        "        tags = [tag.get_text(strip=True) for tag in soup.find_all(\"span\", class_=\"tag-box\")]\n",
        "\n",
        "        time_limit_div = soup.find(\"div\", string=\"time limit per test\")\n",
        "        time_limit = (\n",
        "            time_limit_div.find_next_sibling().get_text(strip=True)\n",
        "            if time_limit_div and time_limit_div.find_next_sibling()\n",
        "            else \"Unknown\"\n",
        "        )\n",
        "\n",
        "        memory_limit_div = soup.find(\"div\", string=\"memory limit per test\")\n",
        "        memory_limit = (\n",
        "            memory_limit_div.find_next_sibling().get_text(strip=True)\n",
        "            if memory_limit_div and memory_limit_div.find_next_sibling()\n",
        "            else \"Unknown\"\n",
        "        )\n",
        "\n",
        "        # Save problem statement\n",
        "        with open(f\"problems/{contest_id}_{problem_id}.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(f\"Problem Title: {problem_title}\\n\")\n",
        "            f.write(f\"Time Limit: {time_limit}\\n\")\n",
        "            f.write(f\"Memory Limit: {memory_limit}\\n\")\n",
        "            f.write(f\"Tags: {', '.join(tags)}\\n\")\n",
        "            f.write(\"\\nProblem Statement:\\n\")\n",
        "            f.write(problem_statement.strip())\n",
        "\n",
        "        # Save metadata\n",
        "        metadata = {\n",
        "            \"contest_id\": contest_id,\n",
        "            \"problem_id\": problem_id,\n",
        "            \"title\": problem_title,\n",
        "            \"tags\": tags,\n",
        "            \"time_limit\": time_limit,\n",
        "            \"memory_limit\": memory_limit,\n",
        "        }\n",
        "        with open(f\"metadata/{contest_id}_{problem_id}.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(metadata, f, indent=4, ensure_ascii=False)\n",
        "\n",
        "        print(f\"Successfully scraped: {problem_title}\")\n",
        "        return True  # Problem successfully scraped\n",
        "\n",
        "    except HTTPError as e:\n",
        "        if e.code == 404:\n",
        "            print(f\"HTTP Error 404: Problem {problem_id} does not exist. Stopping.\")\n",
        "            return False  # Stop processing further problem IDs\n",
        "        print(f\"HTTP Error {e.code}: {url}\")\n",
        "        return False\n",
        "    except URLError as e:\n",
        "        print(f\"URL Error: {e.reason}\")\n",
        "        return False\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")\n",
        "        return False\n",
        "\n",
        "def scrape_contest(contest_id):\n",
        "    problem_ids = \"ABCDEFG\"\n",
        "\n",
        "    for problem_id in problem_ids:\n",
        "        success = scrape_problem(contest_id, problem_id)\n",
        "        if not success:\n",
        "            break  # Stop further scraping if no more problems are found\n",
        "        time.sleep(2)  # Delay between requests\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    contest_id = input(\"Enter a contest ID (e.g., 2043): \").strip()\n",
        "\n",
        "    print(f\"\\nStarting to scrape contest: {contest_id}\")\n",
        "    scrape_contest(contest_id)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IhyH1ItdWo8Y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}