{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LOADING REQUIRED LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import torch\n",
    "import numpy as np\n",
    "import faiss\n",
    "import requests\n",
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_documents(PROBLEM_PATH, EDITORIAL_PATH):\n",
    "\n",
    "    class UTF8TextLoader(TextLoader):\n",
    "        def __init__(self, file_path):\n",
    "            super().__init__(file_path, encoding=\"utf-8\")\n",
    "\n",
    "    loader1 = DirectoryLoader(PROBLEM_PATH, glob=\"*.txt\", loader_cls = UTF8TextLoader)\n",
    "    documents1 = loader1.load()\n",
    "    loader2 = DirectoryLoader(EDITORIAL_PATH, glob=\"*.txt\", loader_cls = UTF8TextLoader)\n",
    "    documents2 = loader2.load()\n",
    "\n",
    "    documents = documents2 + documents1\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EMBEDDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings_data(merged_text):\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(    \n",
    "    chunk_size = 550,\n",
    "    chunk_overlap = 50,\n",
    "    length_function = len,\n",
    "    add_start_index = True,\n",
    "    )\n",
    "\n",
    "    chunks = text_splitter.split_text(merged_text)\n",
    "    print(len(chunks))\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
    "    model = AutoModel.from_pretrained(\"microsoft/codebert-base\")\n",
    "\n",
    "    tokens = tokenizer(chunks, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**tokens)\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "    return embeddings, chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VECTORSTORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorstore(embed, query_embed, k):\n",
    "    np_embed = embed.numpy()\n",
    "    np.save(\"embeddings.npy\", np_embed)\n",
    "    embeddings = np.load(\"embeddings.npy\")\n",
    "\n",
    "    np_query_embed = query_embed.numpy()\n",
    "    np.save(\"query.npy\", np_query_embed)\n",
    "    query_vector = np.load(\"query.npy\")\n",
    "\n",
    "    if len(query_vector.shape) == 1:\n",
    "        query_vector = np.expand_dims(query_vector, axis=0)\n",
    "\n",
    "    index = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "    index.add(embeddings)\n",
    "    faiss.write_index(index, \"vector_store.index\")\n",
    "\n",
    "    index = faiss.read_index(\"vector_store.index\")\n",
    "    distances, indices = index.search(query_vector, k)\n",
    "\n",
    "    return distances, indices\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RETRIEVER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def retrieve_and_generate(tokenizer, model, merged_text, query_text, k):\n",
    "    \n",
    "    embeddings, chunks = generate_embeddings_data(merged_text)\n",
    "\n",
    "    query_tokens = tokenizer(query_text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        query_outputs = model(**query_tokens)\n",
    "    query_embed = query_outputs.last_hidden_state.mean(dim=1)\n",
    "\n",
    "    distances, indices = vectorstore(embeddings, query_embed, k)\n",
    "\n",
    "    top_chunks = [chunks[i] for i in indices[0]]\n",
    "\n",
    "    return top_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CHATBOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98\n",
      "Chatbot Response:\n",
      "\n",
      "Answer the question using the provided context.\n",
      "\n",
      "Context:\n",
      "Problem_ID: 2053D The problem makes no difference when both a and b can be rearranged. Let the rearranged arrays of a and b be c and d respectively. If q=0 , we can write c as SORTED( a 1 , a 2 …, a n ) and d as SORTED( b 1 , b 2 …, b n ) . It can be proved that this reaches the maximum value: if not so, then There must be some pair (i,j) such that c i < c j , d i > d j . Since min( c i , d i )⋅min( c j , d j )= c i ⋅min( c j , d j )≤ c i ⋅min( c j , d i )=min( c i , d j )⋅min( c j , d i ) , we can swap d i and d j , and the product does not\n",
      "Input Specifications:\n",
      "Each test contains multiple test cases. The first line of input contains a single integer tt (1≤t≤1051≤t≤105) — the number of test cases. The description of test cases follows. The only line of each test case contains two integers nn and kk (1≤k≤n≤2⋅1091≤k≤n≤2⋅109).\n",
      "\n",
      "Output Specifications:\n",
      "For each test case, output a single integer — the final lucky value.\n",
      "\n",
      "Problem ID: 2053D\n",
      "Title: D. Refined Product Optimality\n",
      "Time Limit: 3 seconds\n",
      "Memory Limit: 512 megabytes\n",
      "2053D Statement:\n",
      "some rather easier cases below. w≥3 Some initial operations can be conducted a i ≠w We pretend that the final sequence is [w,w,…,w,(w−1),w,w,…,w] , then since ( a i , a i+1 ) must be different after the operation, the last operation can only occur on [w,(w−1)] (or [(w−1),w] ). And since initially a i ≠w , each position must have been operated on at least once. This gives us states such as [w,…,w,x,x,w,…,w] , [w,…,w,y,y,x,w,…,w] , etc. To the leftmost positions, we get a 1 = a 2 (essentially based on Hint 3). Also, we get ...... a n−1 = a n ?\n",
      "is a leaf††, Nora wins‡‡. Whenever qq is a leaf, Aron wins. If either initially both pp and qq are leaves, or after 1010010100 turns the game has not ended, the result is a tie. Please count the number of integer pairs (p,q)(p,q) with 1≤p,q≤n1≤p,q≤n and p≠qp≠q such that, if the caterpillar is initially (p,q)(p,q), Aron wins the game. ∗∗In other words: Let the current caterpillar sequence be c1,c2,…,ckc1,c2,…,ck, then after the move, the new caterpillar sequence becomes d(u,c1),d(u,c2),…,d(u,ck)d(u,c1),d(u,c2),…,d(u,ck). Here, d(x,y)d(x,y) is\n",
      "\n",
      "Question: How do I solve Problem ID 2053D?\n",
      "\n",
      "Answer:\n",
      "Using brute force and sorting, we can solve the problem. It is interesting to note that the problem can be solved using the same strategy as problem ID 2053C.\n",
      "\n",
      "A:\n",
      "\n",
      "For problem 2053D, the idea is to sort the array of the initial caterpillar and the array of the final caterpillar, then find the differences. If the difference is not zero, the caterpillar is not sorted, and therefore Nora wins. Otherwise, if the difference is zero, Aron wins. If the caterpillar is not sorted, it means that the number of moves is less than 1010010100,\n"
     ]
    }
   ],
   "source": [
    "\n",
    "API_URL = \"https://api-inference.huggingface.co/models/bigscience/bloom\" \n",
    "API_TOKEN = \"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\"  \n",
    "\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Bearer {API_TOKEN}\"\n",
    "}\n",
    "\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "Answer the question using the provided context.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "def query_huggingface_api(payload):\n",
    "    \"\"\"Send request to Hugging Face Inference API.\"\"\"\n",
    "    response = requests.post(API_URL, headers=HEADERS, json=payload)\n",
    "    response.raise_for_status()\n",
    "    return response.json()\n",
    "\n",
    "\n",
    "def generate_response_with_prompt(query, context_chunks):\n",
    "    context = \"\\n\".join(context_chunks)\n",
    "    prompt = PROMPT_TEMPLATE.format(context=context, question=query)\n",
    "\n",
    "    payload = {\n",
    "        \"inputs\": prompt,\n",
    "        \"parameters\": {\n",
    "            \"max_new_tokens\":125,\n",
    "            \"temperature\": 0.9,\n",
    "            \"top_p\": 0.7,\n",
    "            \"do_sample\": True\n",
    "        }\n",
    "    }\n",
    "    response = query_huggingface_api(payload)\n",
    "    return response[0][\"generated_text\"]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    query = \"How do I solve Problem ID 2053D?\"\n",
    "    DATA_PATH = \"Project\\data\\problems\"\n",
    "    EDITORIAL_PATH = \"Project\\data\\editorials\"\n",
    "    docs = load_documents(DATA_PATH, EDITORIAL_PATH)\n",
    "    merged_text = \"\\n\".join(doc.page_content for doc in docs)\n",
    "    \n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
    "    model_e = AutoModel.from_pretrained(\"microsoft/codebert-base\")\n",
    "\n",
    "    context_chunks = retrieve_and_generate(tokenizer, model_e, merged_text, query, k=5)\n",
    "    \n",
    "    response = generate_response_with_prompt(query, context_chunks)\n",
    "    print(\"Chatbot Response:\")\n",
    "    print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
